# kafka 升级
## 1.理论
### 1.1 2.0.x to 2.3.0
```
if you are upgrading from 2.0.x to 2.3.0 then a single rolling bounce is needed to swap in the new jar
```
替换jar包

## 2.实操
**伪集群**
### 2.1 安装准备
[kafka_2.11-0.10.0.0.gz](https://archive.apache.org/dist/kafka/0.10.0.0/kafka_2.11-0.10.0.0.tgz)  
[zookeeper-3.4.6.tar.gz](http://archive.apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz)  
[kafka_2.11-2.2.0.tgz](http://mirror.bit.edu.cn/apache/kafka/2.2.0/kafka_2.11-2.2.0.tgz)
[kafka_2.11-2.3.0](https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.3.0/kafka_2.11-2.3.0.tgz)

### 2.2 安装旧版
#### 2.2.1 zookeeper安装
- 安装
>[root@localhost download]# mkdir /data/zookeeper
[root@localhost download]# tar -zxf zookeeper-3.4.6.tar.gz 
[root@localhost download]# mv zookeeper-3.4.6 /data/zookeeper/
[root@localhost download]# mv /data/zookeeper/zookeeper-3.4.6/ /data/zookeeper/zookeeper-1
[root@localhost zookeeper]# cp -r /data/zookeeper/zookeeper-1 /data/zookeeper/zookeeper-2
[root@localhost zookeeper]# cp -r /data/zookeeper/zookeeper-1 /data/zookeeper/zookeeper-3

- 新建各自的 log 和 data 目录
>[root@localhost ~]# mkdir -p /data/zookeeper/tmp/zk1/log
[root@localhost ~]# mkdir -p /data/zookeeper/tmp/zk2/log
[root@localhost ~]# mkdir -p /data/zookeeper/tmp/zk3/log
[root@localhost ~]# mkdir -p /data/zookeeper/tmp/zk1/data
[root@localhost ~]# mkdir -p /data/zookeeper/tmp/zk2/data
[root@localhost ~]# mkdir -p /data/zookeeper/tmp/zk3/data
[root@localhost ~]# touch /data/zookeeper/tmp/zk1/data/myid
[root@localhost ~]# touch /data/zookeeper/tmp/zk2/data/myid
[root@localhost ~]# touch /data/zookeeper/tmp/zk3/data/myid
[root@localhost ~]# echo 1 >> /data/zookeeper/tmp/zk1/data/myid 
[root@localhost ~]# echo 2 >> /data/zookeeper/tmp/zk2/data/myid 
[root@localhost ~]# echo 3 >> /data/zookeeper/tmp/zk3/data/myid

- 配置
>[root@localhost zookeeper]# cd /data/zookeeper/zookeeper-1
[root@localhost zookeeper-1]# cp conf/zoo_sample.cfg conf/zoo.cfg
[root@localhost zookeeper-1]# vim conf/zoo.cfg

依次覆盖一下配置
**zookeeper-1/conf/zoo.cfg**
```
dataDir=/data/zookeeper/tmp/zk1/data
dataLogDir=/data/zookeeper/tmp/zk1/log
clientPort=2181
server.1=127.0.0.1:2887:38887
server.2=127.0.0.1:2888:38888
server.3=127.0.0.1:2889:38889
```
**zookeeper-2/conf/zoo.cfg**
```
dataDir=/data/zookeeper/tmp/zk2/data
dataLogDir=/data/zookeeper/tmp/zk2/log
clientPort=2182
server.1=127.0.0.1:2887:38887
server.2=127.0.0.1:2888:38888
server.3=127.0.0.1:2889:38889
```
**zookeeper-3/conf/zoo.cfg**
```
dataDir=/data/zookeeper/tmp/zk3/data
dataLogDir=/data/zookeeper/tmp/zk3/log
clientPort=2183
server.1=127.0.0.1:2887:38887
server.2=127.0.0.1:2888:38888
server.3=127.0.0.1:2889:38889

```
- 依次启动
>[root@localhost zookeeper]# cd /data/zookeeper
[root@localhost zookeeper]# ./zookeeper-1/bin/zkServer.sh start
JMX enabled by default
Using config: /data/zookeeper/zookeeper-1/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@localhost zookeeper]# ./zookeeper-2/bin/zkServer.sh start
JMX enabled by default
Using config: /data/zookeeper/zookeeper-2/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED
[root@localhost zookeeper]# ./zookeeper-3/bin/zkServer.sh start
JMX enabled by default
Using config: /data/zookeeper/zookeeper-3/bin/../conf/zoo.cfg
Starting zookeeper ... STARTED

- 检查
>[root@localhost zookeeper]# ./zookeeper-1/bin/zkServer.sh status
JMX enabled by default
Using config: /data/zookeeper/zookeeper-1/bin/../conf/zoo.cfg
Mode: follower
[root@localhost zookeeper]# ./zookeeper-2/bin/zkServer.sh status
JMX enabled by default
Using config: /data/zookeeper/zookeeper-2/bin/../conf/zoo.cfg
Mode: leader
[root@localhost zookeeper]# ./zookeeper-3/bin/zkServer.sh status
JMX enabled by default
Using config: /data/zookeeper/zookeeper-3/bin/../conf/zoo.cfg
Mode: follower

#### 2.2.2 安装 kafka_2.11-0.10.0.0.gz
- 安装
>[root@localhost ~]# mkdir /data/kafka
[root@localhost download]# tar -zxf kafka_2.11-0.10.0.0.gz 
[root@localhost download]# mv kafka_2.11-0.10.0.0 /data/kafka/

- 创建三个 broken 的日志文件
>[root@localhost kafka]# mkdir -p /data/kafka/tmp/broken0/kafka-logs
[root@localhost kafka]# mkdir -p /data/kafka/tmp/broken1/kafka-logs
[root@localhost kafka]# mkdir -p /data/kafka/tmp/broken2/kafka-logs

- 配置
>cd /data/kafka/kafka_2.11-0.10.0.0/
>touch server.properties

**server.properties**
```shell
broker.id=0

listeners=PLAINTEXT:/127.0.0.1:9092

num.network.threads=32

num.io.threads=32

socket.send.buffer.bytes=102400

socket.receive.buffer.bytes=102400

socket.request.max.bytes=204857600


log.dirs=/data/kafka-logs

num.partitions=12

num.recovery.threads.per.data.dir=1


log.retention.hours=8

log.segment.bytes=1073741824

log.retention.check.interval.ms=300000

zookeeper.connect=192.168.240.41:2181,192.168.240.41:2182,192.168.240.41:2183,192.168.240.41:2184,192.168.240.41:2185

zookeeper.connection.timeout.ms=1000000

zookeeper.session.timeout.ms=10000

fetch.message.max.bytes=5048576

replica.fetch.max.bytes=5048576

message.max.bytes=5048576

result in data loss

unclean.leader.election.enable=false

min.insync.replicas=2

default.replication.factor=3

delete.topic.enable=true

```
复制多份配置
>[root@localhost kafka_2.11-0.10.0.0]# cp server.properties server01.properties 
[root@localhost kafka_2.11-0.10.0.0]# cp server.properties server02.properties 
[root@localhost kafka_2.11-0.10.0.0]# cp server.properties server03.properties

依次覆盖一下配置
**server01.properties**
```
broker.id=0
# 虚拟机 或者 域名,不同节点端口不一样(真正集群是ip不一样)
listeners=PLAINTEXT://10.1.161.191:9091
# log 地址
log.dirs= /data/kafka/tmp/broken0/kafka-logs
# zookeeper 地址，客户端端口 clientPort
zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183
```

**server03.properties**
```
broker.id=1
# 虚拟机 或者 域名,不同节点端口不一样(真正集群是ip不一样)
listeners=PLAINTEXT://10.1.161.191:9092
# log 地址
log.dirs= /data/kafka/tmp/broken1/kafka-logs
# zookeeper 地址，客户端端口 clientPort
zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183
```

**server02.properties**
```
broker.id=2
# 虚拟机 或者 域名,不同节点端口不一样(真正集群是ip不一样)
listeners=PLAINTEXT://10.1.161.191:9093
# log 地址
log.dirs= /data/kafka/tmp/broken2/kafka-logs
# zookeeper 地址，客户端端口 clientPort
zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183
```
- 整个kafka复制成3份(也可以只有一份，然后启动三次指定不同的配置)
>[root@localhost kafka]# cp -r kafka_2.11-0.10.0.0 kafka_2.11-0.10.0.0-1
[root@localhost kafka]# cp -r kafka_2.11-0.10.0.0 kafka_2.11-0.10.0.0-2
[root@localhost kafka]# cp -r kafka_2.11-0.10.0.0 kafka_2.11-0.10.0.0-3

- 依次启动
>[root@localhost kafka]# ./kafka_2.11-0.10.0.0-1/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-1/server01.properties
[root@localhost kafka]# ./kafka_2.11-0.10.0.0-2/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-2/server02.properties
[root@localhost kafka]# ./kafka_2.11-0.10.0.0-3/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-3/server03.properties

**这时产生了一个 hs_err_pid7257.log 的文件**
```
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 1073741824 bytes for committing reserved memory.
# Possible reasons:
#   The system is out of physical RAM or swap space
#   In 32 bit mode, the process size limit was hit
# Possible solutions:
#   Reduce memory load on the system
#   Increase physical memory or swap space
#   Check if swap backing store is full
#   Use 64 bit Java on a 64 bit OS
#   Decrease Java heap size (-Xmx/-Xms)
#   Decrease number of Java threads
#   Decrease Java thread stack sizes (-Xss)
#   Set larger code cache with -XX:ReservedCodeCacheSize=
# This output file may be truncated or incomplete.
#
#  Out of Memory Error (os_linux.cpp:2657), pid=7257, tid=0x00007fb4d4302700
#
# JRE version:  (8.0_181-b13) (build )
# Java VM: OpenJDK 64-Bit Server VM (25.181-b13 mixed mode linux-amd64 compressed oops)
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
......
```
可知因为内存导致启动失败，为虚拟机添加内存后重新启动即可(虚拟机重启要记得启动zookeeper = =)


- 创建一个 topic 测试一下
>[root@localhost kafka_2.11-0.10.0.0-1]# bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --create --topic my_test_topic --partitions 20 --replication-factor 2
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic "my_test_topic".


### 2.3 升级 kafka - upgrade.from 方式 - 2.2.0
#### 2.3.1 安装新版 kafka
>[root@localhost download]# tar -zxf kafka_2.11-2.2.0.tgz
[root@localhost download]# mv kafka_2.11-2.2.0/ /data/kafka/
[root@localhost download]# cd /data/kafka

- 复制配置文件
[root@localhost kafka]# cp kafka_2.11-0.10.0.0/server01.properties kafka_2.11-2.2.0/
[root@localhost kafka]# cp kafka_2.11-0.10.0.0/server02.properties kafka_2.11-2.2.0/
[root@localhost kafka]# cp kafka_2.11-0.10.0.0/server03.properties kafka_2.11-2.2.0/

- 所有配置文件追加配置 upgrade.fromupgrade.from
>[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-1/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-1/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-1/server03.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-2/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-2/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-2/server03.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-3/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-3/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-3/server03.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-2.2.0/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-2.2.0/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-2.2.0/server03.properties

- 复制三个 broken
>[root@localhost kafka]# cp -r kafka_2.11-2.2.0 kafka_2.11-2.2.0-1
[root@localhost kafka]# cp -r kafka_2.11-2.2.0 kafka_2.11-2.2.0-2
[root@localhost kafka]# cp -r kafka_2.11-2.2.0 kafka_2.11-2.2.0-3

#### 2.3.2 滚动升级
- 滚动重启，生效 upgrade.from="0.10.0" 配置
>[root@localhost kafka]# ps -ef|grep kafka
[root@localhost kafka]# kill -s TERM 17919
[root@localhost kafka]# ./kafka_2.11-0.10.0.0-1/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-1/server01.properties
[root@localhost kafka]# kill -s TERM 13773
[root@localhost kafka]# ./kafka_2.11-0.10.0.0-2/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-2/server02.properties
[root@localhost kafka]# kill -s TERM 14076
[root@localhost kafka]# ./kafka_2.11-0.10.0.0-3/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-3/server03.properties

- 滚动升级
>[root@localhost kafka]# ps -ef|grep kafka
[root@localhost kafka]# kill -s TERM 17919
[root@localhost kafka]# ./kafka_2.11-2.2.0-1/bin/kafka-server-start.sh -daemon kafka_2.11-2.2.0-1/server01.properties 

**新版异常：**
```
[2019-07-16 04:33:18,405] WARN [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=0, maxWait=500, minBytes=1, maxBytes=10485760, fetchData={test_t1-3=(fetchOffset=80, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[7]), test_t1-9=(fetchOffset=86, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[7]), my_test_topic-6=(fetchOffset=0, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[5]), my_test_topic-12=(fetchOffset=0, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[5]), my_test_topic-10=(fetchOffset=0, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[8]), my_test_topic-18=(fetchOffset=0, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[5]), test_t1-0=(fetchOffset=73, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[6]), my_test_topic-16=(fetchOffset=0, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[8]), test_t1-6=(fetchOffset=88, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[6]), test_t1-4=(fetchOffset=84, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[9]), my_test_topic-0=(fetchOffset=0, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[5]), test_t1-10=(fetchOffset=86, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[9]), my_test_topic-4=(fetchOffset=0, logStartOffset=0, maxBytes=5048576, currentLeaderEpoch=Optional[8])}, isolationLevel=READ_UNCOMMITTED, toForget=, metadata=(sessionId=INVALID, epoch=INITIAL)) (kafka.server.ReplicaFetcherThread)
java.io.IOException: Connection to 2 was disconnected before the response was read
	at org.apache.kafka.clients.NetworkClientUtils.sendAndReceive(NetworkClientUtils.java:100)
	at kafka.server.ReplicaFetcherBlockingSend.sendRequest(ReplicaFetcherBlockingSend.scala:100)
	at kafka.server.ReplicaFetcherThread.fetchFromLeader(ReplicaFetcherThread.scala:193)
	at kafka.server.AbstractFetcherThread.kafka$server$AbstractFetcherThread$$processFetchRequest(AbstractFetcherThread.scala:280)
	at kafka.server.AbstractFetcherThread$$anonfun$maybeFetch$1.apply(AbstractFetcherThread.scala:132)
	at kafka.server.AbstractFetcherThread$$anonfun$maybeFetch$1.apply(AbstractFetcherThread.scala:131)
	at scala.Option.foreach(Option.scala:257)
	at kafka.server.AbstractFetcherThread.maybeFetch(AbstractFetcherThread.scala:131)
	at kafka.server.AbstractFetcherThread.doWork(AbstractFetcherThread.scala:113)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
```
### 2.4 升级 kafka - upgrade.from 方式 - 2.3.0
- 安装
>[root@localhost download]# tar -zxf kafka_2.11-2.3.0.tgz 
[root@localhost download]# mv kafka_2.11-2.3.0 /data/kafka/
[root@localhost download]# cd /data/kafka/

- 复制配置
[root@localhost kafka]# cp kafka_2.11-0.10.0.0/server01.properties kafka_2.11-2.3.0/
[root@localhost kafka]# cp kafka_2.11-0.10.0.0/server02.properties kafka_2.11-2.3.0/
[root@localhost kafka]# cp kafka_2.11-0.10.0.0/server03.properties kafka_2.11-2.3.0/

- 追加配置
>[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-1/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-1/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-1/server03.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-2/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-2/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-2/server03.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-3/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-3/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-0.10.0.0-3/server03.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-2.3.0/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-2.3.0/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-2.3.0/server03.properties 

- 复制多份
[root@localhost kafka]# cp -r kafka_2.11-2.3.0 kafka_2.11-2.3.0-1
[root@localhost kafka]# cp -r kafka_2.11-2.3.0 kafka_2.11-2.3.0-2
[root@localhost kafka]# cp -r kafka_2.11-2.3.0 kafka_2.11-2.3.0-3

- 滚动重启，生效 upgrade.from="0.10.0" 配置
>[root@localhost kafka]# ps -ef|grep kafka
[root@localhost kafka]# kill -s TERM 23085
[root@localhost kafka]# ./kafka_2.11-0.10.0.0-1/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-1/server01.properties
[root@localhost kafka]# kill -s TERM 20628
[root@localhost kafka]# ./kafka_2.11-0.10.0.0-2/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-2/server02.properties
[root@localhost kafka]# kill -s TERM 22305
[root@localhost kafka]# ./kafka_2.11-0.10.0.0-3/bin/kafka-server-start.sh -daemon kafka_2.11-0.10.0.0-3/server03.properties


- 滚动升级
>[root@localhost kafka]# ps -ef|grep kafka
[root@localhost kafka]# kill -s TERM 23433
[root@localhost kafka]# ./kafka_2.11-2.3.0-1/bin/kafka-server-start.sh -daemon kafka_2.11-2.3.0-1/server01.properties 
23433
24194
24497


### 2.5 升级 kafka - upgrade.from 方式 - 总结： 失败

### 2.6 升级 kafka 
- 追加配置
>[root@localhost kafka]# echo "inter.broker.protocol.version=0.10.0.0" >> kafka_2.11-2.2.0/server01.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-2.2.0/server02.properties 
[root@localhost kafka]# echo "upgrade.from=\"0.10.0\"" >> kafka_2.11-2.2.0/server03.properties

inter.broker.protocol.version=0.10.2.1
log.message.format.version=0.10.2.1