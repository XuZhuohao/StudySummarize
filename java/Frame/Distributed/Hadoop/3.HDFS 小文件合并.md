# HDFS 小文件合并

## HDFS 小文件
### 小文件？
小文件指的是那些 size 比 HDFS 的 block size (默认64M)小的多的文件。Hadoop适合处理少量的大文件，而不是大量的小文件。

### 小文件导致的问题
1.每个文件占用一个 block ，导致存储空间占用大于实际使用空间。  
2.由于 NameNode 在内存中存储文件的元数据，大量的文件需要大量的运行空间。  
3.内存大小受限于Java heap，同时越高的 heap 启动越缓慢，所以无法纵向扩展，NameNode 内存有限。  
(HDFS 横向扩展方案：HDFS的新特性Federation)

## 小文件解决方案  
1.Hadoop Archive (HAR) (Hadoop自带的解决方案)  
2.Sequence file (Hadoop自带的解决方案)  
3.CombineFileInputFormat (Hadoop自带的解决方案)  


<!-- ------------------------------------分割线------------------------------------ -->
## Hadoop Archive (HAR)
### 概述
Hadoop存档是特殊格式的存档。Hadoop存档映射到文件系统目录。Hadoop归档文件总是带有* .har扩展名  
Hadoop存档目录包含元数据（采用_index和_masterindex形式）  
数据部分data（part- *）文件。  
_index文件包含归档文件的名称和部分文件中的位置。   

### 如何创建档案
**1.archive 指令：**  
  用法： hadoop archive -archiveName  归档名称 -p 父目录 [-r <复制因子>]  原路径（可以多个）  目的路径  
  -archivename是您想要创建的档案的名称。该名称应该有一个* .har扩展名。父参数是指定文件应归档到的相对路径  
2.例子：  
  hadoop archive -archiveName foo.har -p /foo/bar -r 3  a b c /user/hz  
**执行该命令后，原输入文件不会被删除，需要手动删除**  
  hadoop fs -rmr /foo/bar/a  
  hadoop fs -rmr /foo/bar/b  
  hadoop fs -rmr /foo/bar/c  
  这里/foo/ bar是父路径，  
  a b c (路径用空格隔开，可以配置多个)是父路径的相对路径。  
  请注意，这是一个创建档案的Map/Reduce作业。你需要一个map reduce集群来运行它。  
  -r表示期望的复制因子; 如果未指定此可选参数，则将使用复制因子10。  
  目的存档一个目录/user/hz  
**请注意档案是不可变的**

### 如何在档案中查找文件 
**该档案将自己公开为文件系统层。因此，档案中的所有fs shell命令都可以工作，但使用不同的URI**  
1.Hadoop Archives的URI是：  
  HAR：/方案-主机名：端口/archivepath/fileinarchive  
  如果没有提供方案，它假定底层文件系统。在这种情况下，URI看起来像  
  HAR:/archivepath/fileinarchive  
例如：
  har:/user/admin/test3.har

### 如何解除归档
1.依次取消存档：  
  hadoop fs -cp har:/user/admin/test3.har /user/admin/oo  
2.要并行解压缩，请使用DistCp：  
  hadoop distcp har:/user/admin/test3.har /user/admin/oo2   

### 注意事项
**注意：**
第一，对小文件进行存档后，原文件并不会自动被删除，需要用户自己删除；  
第二，创建HAR文件的过程实际上是在运行一个mapreduce作业，因而需要有一个hadoop集群运行此命令。  
**小心(坑)：**
第一，一旦创建，Archives便不可改变。要增加或移除里面的文件，必须重新创建归档文件。  
第二，要归档的文件名中不能有空格，否则会抛出异常，可以将空格用其他符号替换(使用-Dhar.space.replacement.enable=true 和-Dhar.space.replacement参数)。  
第三，存档文件不支持压缩。  


<!-- ------------------------------------分割线------------------------------------ -->
## Sequence file
sequence file由一系列的二进制key/value组成，如果为key小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。  
Hadoop-0.21.0中提供了SequenceFile，包括Writer，Reader和SequenceFileSorter类进行写，读和排序操作。hadoop版本必须不低于0.21.0的版本  

### 写操作
**SequenceFileWriteDemo**  
```
// cc SequenceFileWriteDemo Writing a SequenceFile
import java.io.IOException;
import java.net.URI;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.Text;
 
// vv SequenceFileWriteDemo
public class SequenceFileWriteDemo {
  
  private static final String[] DATA = {
    "One, two, buckle my shoe",
    "Three, four, shut the door",
    "Five, six, pick up sticks",
    "Seven, eight, lay them straight",
    "Nine, ten, a big fat hen"
  };
  
  public static void main(String[] args) throws IOException {
    String uri = args[0];
    Configuration conf = new Configuration();
    FileSystem fs = FileSystem.get(URI.create(uri), conf);
    Path path = new Path(uri);
 
    IntWritable key = new IntWritable();
    Text value = new Text();
    SequenceFile.Writer writer = null;
    try {
      writer = SequenceFile.createWriter(fs, conf, path,
          key.getClass(), value.getClass());
      
      for (int i = 0; i < 100; i++) {
        key.set(100 - i);
        value.set(DATA[i % DATA.length]);
        System.out.printf("[%s]\t%s\t%s\n", writer.getLength(), key, value);
        writer.append(key, value);
      }
    } finally {
      IOUtils.closeStream(writer);
    }
  }
}
// ^^ SequenceFileWriteDemo
```

```
package org.apache.hadoop.io;
 
import java.io.IOException;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.SequenceFile.Writer;
import org.apache.hadoop.io.SequenceFile.Writer.FileOption;
import org.apache.hadoop.io.SequenceFile.Writer.KeyClassOption;
import org.apache.hadoop.io.SequenceFile.Writer.ValueClassOption;
import org.apache.hadoop.io.Text;
 
public class THT_testSequenceFile2 {
 
	private static final String[] DATA = { "One, two, buckle my shoe",
			"Three, four, shut the door", "Five, six, pick up sticks",
			"Seven, eight, lay them straight", "Nine, ten, a big fat hen" };
 
	public static void main(String[] args) throws IOException {
		// String uri = args[0];
		String uri = "file:///D://B.txt";
		Configuration conf = new Configuration();
		Path path = new Path(uri);
 
		IntWritable key = new IntWritable();
		Text value = new Text();
		SequenceFile.Writer writer = null;
		SequenceFile.Writer.FileOption option1 = (FileOption) Writer.file(path);
		SequenceFile.Writer.KeyClassOption option2 = (KeyClassOption) Writer.keyClass(key.getClass());
		SequenceFile.Writer.ValueClassOption option3 = (ValueClassOption) Writer.valueClass(value.getClass());
		
		try {
			
			writer = SequenceFile.createWriter( conf, option1,option2,option3,Writer.compression(CompressionType.RECORD));
			
			for (int i = 0; i < 10; i++) {
				key.set(1 + i);
				value.set(DATA[i % DATA.length]);
				System.out.printf("[%s]\t%s\t%s\n", writer.getLength(), key,
						value);
				writer.append(key, value);
			}
		} finally {
			IOUtils.closeStream(writer);
		}
	}
}
```
### 读操作
```

package org.apache.hadoop.io;
 
import java.io.IOException;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.SequenceFile;
import org.apache.hadoop.io.SequenceFile.Reader;
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.util.ReflectionUtils;
 
public class THT_testSequenceFile3 {
 
	public static void main(String[] args) throws IOException {
		//String uri = args[0];
		String uri = "file:///D://B.txt";
		Configuration conf = new Configuration();
		Path path = new Path(uri);
		SequenceFile.Reader.Option option1 = Reader.file(path);
		SequenceFile.Reader.Option option2 = Reader.length(174);//这个参数表示读取的长度
		
		SequenceFile.Reader reader = null;
		try {
			reader = new SequenceFile.Reader(conf,option1,option2);
			Writable key = (Writable) ReflectionUtils.newInstance(
					reader.getKeyClass(), conf);
			Writable value = (Writable) ReflectionUtils.newInstance(
					reader.getValueClass(), conf);
			long position = reader.getPosition();
			while (reader.next(key, value)) {
				String syncSeen = reader.syncSeen() ? "*" : "";
				System.out.printf("[%s%s]\t%s\t%s\n", position, syncSeen, key,
						value);
				position = reader.getPosition(); // beginning of next record
			}
		} finally {
			IOUtils.closeStream(reader);
		}
	}
}
```

## CombineFileInputFormat






