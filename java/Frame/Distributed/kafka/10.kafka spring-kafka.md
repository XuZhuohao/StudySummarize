# spring-kafka

## 1. 依赖

```xml
<!-- kafka -->
<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
    <version>2.3.4.RELEASE</version>
</dependency>
<!-- 工具 lombok（可要可不要） -->
<dependency>
    <groupId>org.projectlombok</groupId>
    <artifactId>lombok</artifactId>
    <version>1.18.2</version>
</dependency>
```



## 2. 配置类型简易实现

### 2.1 默认配置-consumer

```java
package com.yui.config.kafka;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;

import java.util.Map;

/**
 * @author XuZhuohao
 * @date 2020/04/08
 */
@Data
@Configuration
@ConfigurationProperties("mq.kafka.consumer")
public class KafkaConsumerProperties {
    private Map<String, String> other;
    private String clusters;
    private String groupId;
    private boolean enableAutoCommit = true;
    private String sessionTimeoutMs = "120000";
    private String requestTimeoutMs = "160000";
    private String fetchMaxWaitMs = "5000";
    private String autoOffsetReset = "earliest";
    private String maxPollRecords = "50";
    private String maxPartitionFetch = "5048576";
    private String keyDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer";
    private String valueDeserializerClass = "org.apache.kafka.common.serialization.StringDeserializer";
}
```



### 2.2 默认配置-Producer

```java
package com.yui.config.kafka;

import lombok.Data;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.context.annotation.Configuration;

import java.util.Map;

/**
 * @author XuZhuohao
 * @date 2020/04/08
 */
@Data
@Configuration
@ConfigurationProperties("mq.kafka.producer")
public class KafkaProducerProperties {
    /**
     * 其他配置
     */
    private Map<String, String> other;
    /**
     * 集群地址
     */
    private String clusters;
    /**
     * 0	        Producer 往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。
     * 1	        Producer 往集群发送数据只要 Leader 应答就可以发送下一条，只确保 Leader 接收成功。
     * -1 或 all    Producer 往集群发送数据需要所有的ISR Follower 都完成从 Leader 的同步才会发送下一条，
     *              确保 Leader 发送成功和所有的副本都成功接收。安全性最高，但是效率最低。
     */
    private String acks = "all";
    /**
     * 重试次数
     */
    private int retries = 0;
    /**
     * 将同分区数据合并为一个 batch 发送， 单位 bytes
     */
    private String batchSize = "16384";
    /**
     * 发送延迟时间，当 batch 未满 batchSize 时，延迟 lingerMs 发送 batch
     * 当数据达到 batchSize 将会被立即发送
     */
    private String lingerMs = "1";
    /**
     * 缓冲大小，当缓冲满时，max.block.ms 会控制 send() && partitionsFor() 阻塞时长
     */
    private String bufferMemory = "33554432";
    /**
     * bytes
     */
    private String maxRequestSize = "5048576";
    /**
     * 压缩方式 none, gzip, snappy, lz4, or zstd.
     */
    private String compressionType = "gzip";
    /**
     * key序列化方式，实现 org.apache.kafka.common.serialization.StringSerializer 的类
     */
    private String keySerializerClass = "org.apache.kafka.common.serialization.StringSerializer";
    /**
     * value序列化方式，实现 org.apache.kafka.common.serialization.StringSerializer 的类
     */
    private String valueSerializerClass = "org.apache.kafka.common.serialization.StringSerializer";
}

```



### 2.3 yaml 文件配置

```yml
mq:
  kafka:
    consumer:
      # kafka 默认 group
      groupId: kafka-server
      # kafka comsumer 集群
      clusters: 192.168.240.62:9092,192.168.240.62:9093,192.168.240.62:9094
    producer:
      # kafka producer 集群
      clusters: 192.168.240.62:9092,192.168.240.62:9093,192.168.240.62:9094
    topic: kafka-test
```



### 2.4 构建配置

```java
package com.yui.config.kafka;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.*;

import java.util.HashMap;
import java.util.Map;

/**
 * @author XuZhuohao
 * @date 2020-04-06 14:41
 */
@Configuration
@EnableKafka
public class KafkaConfig {
    @Autowired
    private KafkaProducerProperties producerProperties;

    @Autowired
    private KafkaConsumerProperties consumerProperties;

	/**
     * bean name 为 kafkaListenerContainerFactory 是必须的 
     * 在 {@link org.springframework.boot.autoconfigure.kafka.KafkaAnnotationDrivenConfiguration#kafkaListenerContainerFactory} 中
     * 使用 @ConditionalOnMissingBean(name = "kafkaListenerContainerFactory") 对没有该 bean 的情况下进行处理
     * 走默认配置，即 spring 的配置 spring.kafka.*
     */
    @Bean
    ConcurrentKafkaListenerContainerFactory<Integer, String>
    kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<Integer, String> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        // //设置为批量消费，每个批次数量在Kafka配置参数中设置ConsumerConfig.MAX_POLL_RECORDS_CONFIG
        factory.setBatchListener(true);
        return factory;
    }

    @Bean
    public ConsumerFactory<Integer, String> consumerFactory() {
        return new DefaultKafkaConsumerFactory<>(consumerConfigs());
    }

    @Bean
    public Map<String, Object> consumerConfigs() {
        Map<String, Object> props = new HashMap<>(16);
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, consumerProperties.getClusters());
        props.put(ConsumerConfig.GROUP_ID_CONFIG, consumerProperties.getGroupId());
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, consumerProperties.isEnableAutoCommit());
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, consumerProperties.getSessionTimeoutMs());
        props.put(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG, consumerProperties.getRequestTimeoutMs());
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, consumerProperties.getFetchMaxWaitMs());
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, consumerProperties.getAutoOffsetReset());
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, consumerProperties.getMaxPollRecords());
        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, consumerProperties.getMaxPartitionFetch());
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, consumerProperties.getKeyDeserializerClass());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, consumerProperties.getValueDeserializerClass());
        if (consumerProperties.getOther() != null){
            props.putAll(consumerProperties.getOther());
        }
        return props;
    }

    @Bean
    public KafkaProperties.Listener listener() {
        return new KafkaProperties.Listener();
    }

    @Bean
    public ProducerFactory<Integer, String> producerFactory() {
        return new DefaultKafkaProducerFactory<>(producerConfigs());
    }

    @Bean
    public Map<String, Object> producerConfigs() {
        Map<String, Object> props = new HashMap<>(16);
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, producerProperties.getClusters());
        props.put(ProducerConfig.ACKS_CONFIG, producerProperties.getAcks());
        props.put(ProducerConfig.RETRIES_CONFIG, producerProperties.getRetries());
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, producerProperties.getBatchSize());
        props.put(ProducerConfig.LINGER_MS_CONFIG,  producerProperties.getLingerMs());
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, producerProperties.getBufferMemory());
        props.put(ProducerConfig.MAX_REQUEST_SIZE_CONFIG, producerProperties.getMaxRequestSize());
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, producerProperties.getCompressionType());
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, producerProperties.getKeySerializerClass());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, producerProperties.getValueSerializerClass());
        if (producerProperties.getOther() != null){
            props.putAll(producerProperties.getOther());
        }
        return props;
    }

    @Bean
    public KafkaTemplate<Integer, String> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }

}

```



- kafkaListenerContainerFactory() 关系到消费监听配置
- kafkaTemplate() 为发送使用服务
- 其他方法为上述两个服务服务



### 2.5 生产者

**interface**

```java
package com.yui.service;

/**
 * @author XuZhuohao
 * @date 2020/04/08
 */
public interface KafkaService {
    /**
     * 消息发送接口
     * @param topic topic
     * @param message message
     */
    void sendMessage(String topic, String message);
}

```



**callback**

```java
package com.yui.listen;

import com.alibaba.fastjson.JSON;
import lombok.extern.slf4j.Slf4j;
import org.springframework.kafka.support.SendResult;
import org.springframework.util.concurrent.ListenableFutureCallback;

/**
 * kafka 发送回调
 *
 * @author XuZhuohao
 * @date 2020/04/08
 */
@Slf4j
public class KafkaSendCallback implements ListenableFutureCallback<SendResult<Integer, String>> {
    private String topic;
    private String message;

    public KafkaSendCallback(String topic, String message) {
        this.topic = topic;
        this.message = message;
        this.isOrderly = isOrderly;
    }

    @Override
    public void onFailure(Throwable throwable) {
        throwable.printStackTrace();
        log.error("kafka 数据发送失败：topic:{},message:{} \r\n 失败原因：{}",
                topic, message, throwable.getMessage());
    }

    @Override
    public void onSuccess(SendResult<Integer, String> sendResult) {
        log.debug("kafka 数据发送成功：{}",sendResult.toString());
    }
}
```



**implements**

```java
package com.yui.service.impl;

import com.alibaba.fastjson.JSON;
import com.yui.listen.KafkaSendCallback;
import com.yui.service.KafkaService;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.SendResult;
import org.springframework.stereotype.Service;
import org.springframework.util.concurrent.ListenableFuture;

/**
 * @author XuZhuohao
 * @date 2020/04/08
 */
@Service
public class KafkaServiceImpl implements KafkaService {
    @Autowired
    private KafkaTemplate<Integer, String> kafkaTemplate;

    @Override
    public void sendMessage(String topic, String message) {
        // 消息发送
        ListenableFuture<SendResult<Integer, String>> send = kafkaTemplate.send(topic, message);
        // 结果监听
        send.addCallback(new KafkaSendCallback(topic, message));
    }
}
```



### 2.6 消费者

```java
package com.yui.listen;

import com.alibaba.fastjson.JSON;
import lombok.extern.slf4j.Slf4j;
import org.apache.commons.lang.exception.ExceptionUtils;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

/**
 * @author XuZhuohao
 * @date 2020/04/08
 */
@Slf4j
@Component
public class KafkaMessageListener {
    @Autowired
    private DataSyncService dataSyncService;
    @Autowired
    private DataSyncConfig dataSyncConfig;
    @Autowired
    private SyncDataErrLogService syncDataErrLogService;

    /**
     * 数据回写，读取 kafka 数据]
     * @param records 数据集
     */
    @KafkaListener(topics = "${mq.kafka.topic}", groupId = "${mq.kafka.consumer.groupId}")
    public void recoverMessageToDataSync(ConsumerRecords<Integer, String> records) {
        log.info("poll recoverMessageToDataSync：count:{}", records.count());
        for (ConsumerRecord<Integer, String> record : records) {
            log.info("kafka 数据 {}", record.value());
        }
    }

}
```





## 3. 一个项目对接多个集群

### 3.1 消费者

#### 3.1.1 修改监听实现

```java
    // @Primary 同个类（接口）多个实现时，@Primary 标志默认实现
    @Bean
    @Primary
    ConcurrentKafkaListenerContainerFactory<Integer, String>
    kafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<Integer, String> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory());
        // //设置为批量消费，每个批次数量在Kafka配置参数中设置ConsumerConfig.MAX_POLL_RECORDS_CONFIG
        factory.setBatchListener(true);
        return factory;
    }
    @Bean
    ConcurrentKafkaListenerContainerFactory<Integer, String>
    kafkaListenerContainerFactory02() {
        ConcurrentKafkaListenerContainerFactory<Integer, String> factory =
                new ConcurrentKafkaListenerContainerFactory<>();
        Map<String, Object> map = consumerConfigs();
        map.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "127.0.0.1:13145");
        factory.setConsumerFactory(new DefaultKafkaConsumerFactory<>(map));
        // //设置为批量消费，每个批次数量在Kafka配置参数中设置ConsumerConfig.MAX_POLL_RECORDS_CONFIG
        factory.setBatchListener(true);
        return factory;

    }
```



#### 3.1.2 修改监听器

使用 containerFactory 来声明注入对象

```java
@KafkaListener(topics = "${gateway.data.sync.triggerTopic}", groupId = "${bm.mq.kafka.consumer.groupId}",containerFactory = "kafkaListenerContainerFactory02")
```



### 3.2 生产者

添加 kafkaTemplate 不同实现的 bean 就可以。这个没什么好说的。