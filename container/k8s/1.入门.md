

## 1. 概述

Infrastructure as a Service : 阿里云

platform as a service : 新浪云

Software as a service : office 365



- 资源管理器
  - MESOS
  - docker SWARM
  - kubernetes

### 1.1 知识结构

- 发展历史
- 组件说明：框架，关键字含义
- 基础概念
  - pod 概念
  - 通信模式
- 安装
  - 系统初始化
  - 部署
  - 常见问题
- 资源清单
  - 资源的概念
  - 资源清单
  - 通过清单编写 pod
  - **pod 的生命周期**
- **pod 控制器**
- 服务发现
  - Service 原理
  - Ingress
- 存储
  - volume
  - PV
  - Secret
  - configMap
- 调度器
  - 概念
  - 调度亲和性
- 集群安全机制
- HELM
- 运维



### 1.2 组件

**架构图**

![k8s架构图](https://raw.githubusercontent.com/XuZhuohao/picture/master/container/k8s/K8S%E6%9E%B6%E6%9E%84%E5%9B%BE.png)

- master
  - api server：所有服务访问统一入口
  - ControllerManager： 维护副本期望数目
  - Scheduler： 负责接受任务，选择合适的节点进行分配任务
  - etcd： 键值对数据库，存储k8s集群所有重要信息（持久化）
- node
  - Kubelet：直接跟容器引擎交互实现容器的生命周期管理
  - Kube-proxy：负责写入规则到 IPTABLES、IPVS 实现服务映射访问
- 插件
  - CoreDNS：可以为集群中的 svc 创建一个域名 ip 的对应关系解析
  - Dashboard： 给 k8s 集群提供一个 B/S 结构访问体系
  - Ingress Controller：官方只能实现四层代理，Ingress 可以实现七层代理
  - Federation：提供一个可以跨集群中心多 K8S 统一管理功能
  - Prometheus：提供一个 K8S 集群的监控能力
  - ELK：提供一个 K8S 集群日志统一分析接入平台







#### 1.2.1 etcd

etcd 是一个可信赖的分布式键值存储服务

- v2: Memory
- v3: Database

##### 1.2.1.1 架构图

![etcd 架构图](https://raw.githubusercontent.com/XuZhuohao/picture/master/container/k8s/etcd%E6%9E%B6%E6%9E%84%E5%9B%BE.png)



##### 1.2.1.2 Raft 

`Raft`是用于**管理复制日志的一致性算法**。

它的效果相当于(multi-)Paxos，跟Paxos一样高效，但结构与Paxos不同。



###### 关于一致性问题

- cap 理论：
  - **一致性（C）**：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）
  - 可用性（A）：保证每个请求不管成功或者失败都有响应。
  - 分区容忍性（P）：系统中任意信息的丢失或失败不会影响系统的继续运作
- 一致性是分布式系统容错的基本问题。一致性涉及**多个服务器状态（Values）达成一致**。



###### raft 基础知识

- `Raft 集群`包含多个服务器，5个服务器是比较典型的，允许系统容忍两个故障

- 集群服务器有三个状态：

  - `Leader`: 领导者，处理所有客户端交互，日志复制等，一般一次只有一个Leader
  - `Follower`: 追随者，类似选民，完全被动
  - `Candidate`: 候选人，可以被选为·一个新的领导人

- 选举 leader

  - 使用**心跳机制**来触发领导者选举

  ```mermaid
  graph TD
      A(开始) --> B(以Flower身份启动)
      B --> C{指定时间内<br>是否接收到<br>Leader心跳}
  	C -.yes--> D(结束)
  	C -.no--> F(Flower 晋升为 Candidate)
  	F --> G(发起投票)
  	G --> H{是否选举超时}
  	H -.yes--> G
  	H -.no--> I(收到大部分选票<br>成为Leader)
  	H -.no--> J(收到Leader心跳<br>成为Flower)
  	J --> B
  	I --> K(结束)
  	
  ```

  - 一个任期内只能投一次票
  - 每次 rpc 请求中都会携带 term 值(任期)

- 日志复制

  - `Leader` 处理客户端请求
  - 每个客户端请求包含由复制状态机执行的命令
  - `Leader` 向每个其他服务器发出`AppendEntries RPC`以复制条目





## 2. Pod

pause

### 2.1 pod 控制类型

#### 2.1.1 ReplicationController(RC) & ReplicaSet(RS)

##### - **RC**

用来确保容器应用的副本数始终保持在用户定义的副本数

- 有容器异常退出，自动创建新的 pod 来代替
- 异常多出来的容器也会呗自动回收

**在新版的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController**



##### - RS

`RS` 和 `RC` 没有本质的不同，只是名字不一样，并且 `RS` 支持 **集合式的 selector**

- `RS` 虽然可以队里使用，但是一般建议使用 `Deployment` 来自动管理 `RS` ，这样就不用担心和其他机制不兼容的问题（如： RS 不支持 rolling-update）



#### 2.1.2  HPA (HorizontalPodAutoScale)

Horizontal Pod Autoscaling 仅适用于 `Deployment` 和 `ReplicaSet` ，在 V1 版本中仅支持工具 Pod 的 CPU 利用率扩缩容，在 vlalpha 版本中，支持根据内存和用户自定义的 metric 扩缩容



#### 2.1.3 StatefulSet

`StatefulSet` 是为了解决有状态服务的问题（对应  `Deployment` 和 `ReplicaSet` 是为了无状态服务而设计）

- 应用场景：
  - **稳定的持久化存储**，即 `Pod` 重新调度后还是能访问到相同的持久化数据，基于 `PVC` 来实现
  - **稳定的网络标志**，即 Pod 重新调度后其 `PodName` 和 `HostName` 不变，基于`Headless Service`
    （即没有 Cluster IP 的 Service）来实现
  - **有序部署**，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进（即从0到N-1，在下一个 Pod 运行之前所有之前的 Pod 必须都是 `Running` 和 `Ready` 状态），基于 `init containers` 来实现
  - **有序收缩**，有序删除（即从N-1到0）



#### 2.1.4 DaemonSet

`DaemonSet` 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个Pod。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod 使用

- 典型用法：
  - 运行集群存储 daemon，例如在每个Node上运行 glusterd、ceph。
  - 在每个 Node 上运行日志收集 daemon，例如 fluentd、logstash。
  - 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter



#### 2.1.5 Job, CronJob

`Job` 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束

`Cron Job`管理基于时间的Job，即：

- 在给定时间点只运行一次
- 周期性地在给定时间点运行



### 2.2 网络通讯模式

